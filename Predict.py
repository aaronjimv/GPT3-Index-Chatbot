import openai

# Set up the OpenAI API key
openai.api_key = "my_key"

from llama_index import SimpleDirectoryReader, GPTListIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper, ServiceContext
from llama_index import StorageContext, load_index_from_storage
import os
import gradio as gr 
from langchain.embeddings import OpenAIEmbeddings
from langchain import OpenAI

# Set up the OpenAI API key as an environment variable
os.environ['OPENAI_API_KEY'] = openai.api_key

def chatbot(question):
    """
    Respond to a question using the previously trained index.

    Args:
        question (str): Question input by the user.

    Returns:
        str: Response generated by the chatbot.
    """
    storage_context = StorageContext.from_defaults(persist_dir='Modelo')
    index = load_index_from_storage(storage_context)    
    query_engine = index.as_query_engine()
    resonse = query_engine.query(question)
    return resonse

# Graphical interface for the chatbot using Gradio
app = gr.Interface(fn=chatbot,
                   inputs= gr.inputs.Textbox(lines=5, label="Ingrese una peticion"),
                   outputs= "text",
                   title="Chatbot")

# Launch the graphical interface
app.launch()